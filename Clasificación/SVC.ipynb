{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://ichi.pro/assets/images/max/724/0*INqwwHXgTabQx7wM.png\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://ichi.pro/assets/images/max/724/0*INqwwHXgTabQx7wM.png</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Máquinas de Vector Soporte (SVC) para Clasificación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es?**\n",
    "\n",
    "El objetivo de este algoritmo es encontrar un hiperplano un un espacio N-dimensional (N = número de variables) que clasifica datos\n",
    "\n",
    "Se busca encontrar un hiperplano que tenga el margen máximo (máxima distancia) entre los puntos de ambas clases. \n",
    "\n",
    "<img style=\"float: center; margin: 15px 15px 0px 0px;\" src=\"https://miro.medium.com/max/600/0*9jEWNXTAao7phK-5.png\" width=\"250px\" height=\"80px\" />\n",
    "\n",
    "<img style=\"float: center; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/600/0*0o8xIA4k3gXUDCFU.png\" width=\"250px\" height=\"80px\" />\n",
    "\n",
    "\n",
    "**¿En qué conceptos se fundamenta?**\n",
    "\n",
    "- En el \"Maximal Margin Classifier\"\n",
    "- En el concepto de hiperplano\n",
    "\n",
    "**Su ecuación**\n",
    "\n",
    "$$\n",
    "\\min _{\\mathbf{w}, b}\\|\\mathbf{w}\\|^{2}+C \\sum_{i=1}^{n} \\xi_{i}^{p} \\text { under constraints } y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1-\\xi_{i}, \\quad \\xi_{i} \\geq 0\n",
    "$$\n",
    "\n",
    "\n",
    "**El truco del Kernel**\n",
    "\n",
    "No todos los datos son linealmente separables, casi todos los datos están aleatoriamente distribuidos, lo cual hace difícil separar linealmente los datos. \n",
    "\n",
    "<img style=\"float: center; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/838/1*gXvhD4IomaC9Jb37tzDUVg.png\" width=\"450px\" height=\"280px\" />\n",
    "\n",
    "En escencia, lo que hace el truco del kernel es ofrecer una manera más eficiente y menos costosa de aumentar la dimensionalidad de los datos. \n",
    "\n",
    "**Algunos tipos de kernel**\n",
    "- Lineal\n",
    "$$K(x,x*)=x \\cdot x^{*}$$\n",
    "- Polinomial \n",
    "$$K(x,x*)=(x \\cdot x^{*} +1)^{d}$$\n",
    "- Gaussian Radial Basis \n",
    "$$K(x,x*)=\\exp^{-\\frac{\\|x-x^{*}\\|^{2}}{2\\sigma^{2}}}=\\exp^{-\\gamma\\|x-x^{*}\\|^{2}} $$\n",
    "\n",
    "**Se deben cumplir varias cosas para obtener el resultado óptimo:**\n",
    "\n",
    "- Este algoritmo funciona muy bien si se tienen los datos limpios. Si los datos están muy dispersos no se podrá crear una fórmula adecuada. Se recomienda estandarizar los datos previamente. \n",
    "- No es adecuado para conjuntos de datos grandes. Lleva mucho tiempo el entrenamiento\n",
    "- Menos efectivo en conjuntos de datos con columnas superpuestas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observando qué hace la máquina\n",
    "#### Ejemplo 1 Kernel lineal y suavisando los márgenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasaría si tenemos datos no tan limpios y tenemos clases que se sobreponen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2) # por default hace 2 \"x\"\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Graficar la función de decisón para un SVC en dos dimensiones\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # crear malla para evaluar el modelo\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # graficar el límite de decisión y los márgenes\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # graficar vectores de soporte\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manejar este caso, a máquina de SVM tiene un factor que \"suaviza\" el margen. Osea que permite que algunos puntos sean mal clasificados de manera que se predigan mejor los datos. \n",
    "\n",
    "La dureza del margen es controlada por un parámetro, conocido como \"C\". \n",
    "\n",
    "Con una \"C\" muy grande, el margen es duro y no permite errores de clasificación. \n",
    "\n",
    "Con una \"C\" más pequeña, el margen es más suave y permite algunos errores de clasificación. \n",
    "\n",
    "El gráfico siguiente muestra visualmente cómo diferentes valores de \"C\" afectan al ajuste final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos la máquina de vector soporte\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "#probar con dos diferentes parámetros (C=10, C=0.1)\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor óptimo de C depende de los datos, y debe de ser tuneado usando cross-validation o algún método similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2 varios tipos de kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model,svm\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score)\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación de performance\n",
    "def eval_perform(Y,Yhat):\n",
    "    accu = accuracy_score(Y,Yhat)\n",
    "    prec = precision_score(Y,Yhat,average='weighted')\n",
    "    reca = recall_score(Y,Yhat,average='weighted')\n",
    "    print('\\n \\t Accu \\t Prec \\t Reca\\n Eval \\t %0.3f \\t %0.3f \\t %0.3f'%(accu,prec,reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar datos\n",
    "data = pd.read_csv('ex2data2.txt',header=None)\n",
    "X = data.iloc[:,0:2]\n",
    "Y = data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de los datos\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "indx = Y==1\n",
    "plt.scatter(X[0][indx],X[1][indx],c='g',label='Class: +1')\n",
    "plt.scatter(X[0][~indx],X[1][~indx],c='r',label='Class: -1')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el modelo\n",
    "modelo = svm.SVC(kernel='linear')\n",
    "#modelo = svm.SVC(kernel='poly',degree=2,C=1)\n",
    "#modelo = svm.SVC(kernel='rbf',C=1000,gamma='auto') #rbf #radial\n",
    "modelo.fit(X,Y)\n",
    "\n",
    "Yhat = modelo.predict(X)\n",
    "\n",
    "eval_perform(Y,Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver el límite de decisión\n",
    "h = 0.01\n",
    "xmin,xmax,ymin,ymax = X[0].min(),X[0].max(),X[1].min(),X[1].max()\n",
    "xx,yy = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "\n",
    "Xnew = pd.DataFrame(np.c_[xx.ravel(),yy.ravel()])\n",
    "\n",
    "Z = modelo.predict(Xnew)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "vs = modelo.support_vectors_\n",
    "\n",
    "indx = Y==1\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(X[0][indx],X[1][indx],c='g',label='Class: +1')\n",
    "plt.scatter(X[0][~indx],X[1][~indx],c='r',label='Class: -1')\n",
    "plt.contour(xx,yy,Z)\n",
    "plt.scatter(vs[:,0],vs[:,1],s=10,facecolors='k')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.legend()\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: Reconocimiento Facial\n",
    "\n",
    "Vamos a tomar un problema de reconocimiento facial. Usaremos unos datos que consisten en miles de fotos recolectadas de varias figuras públicas, está disponible en Scikit-Learn. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "#Vamos a obtener las caras de personas que tienen al menos 60 fotografías\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faces.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cada imagen tiene un tamaño original de 62 x 47 pixeles\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, h, w = faces.images.shape\n",
    "n_classes = faces.target_names.shape[0]\n",
    "X = faces.data\n",
    "n_features = X.shape[1]\n",
    "print(\"Tamaño del dataset:\")\n",
    "print(\"# observaciones: %d\" % n_samples)\n",
    "print(\"# variables: %d\" % n_features)\n",
    "print(\"# clases: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos algunas caras   \n",
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_xlabel(faces.target_names[faces.target[i]], fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen contiene [62x47] o 3000 pixeles aprox. Podríamos usar cada pixel como variable, pero ya que serían demasiadas variables vamos a preprocesar los datos para extraer variables más importantes. \n",
    "\n",
    "Utilizaremos el análisis de componentes principales para extraer 150 componentes que alimenten nuestra máquina. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero dividimos nuestros datos en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = faces.data\n",
    "y = faces.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=150, whiten=True).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "#extraemos los eigenvectores\n",
    "eigenfaces = pca.components_.reshape((150, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el método de grid search para ajustar los hiperparámetros. \n",
    "Aquí vamos a ajustar dos cosas:\n",
    "\n",
    "- Ajustamos el parámetro \"C\" (controla la dureza del margen)\n",
    "- Ajustamos el parámetro \"Gamma\" (controla el tamaño de la función del kernel de base radial)\n",
    "\n",
    "Y determinamos el mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos el método de grid search para ajustar los hiperparámetros\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]} #por lo general son valores menores a 1\n",
    "\n",
    "model = SVC(kernel='rbf', class_weight='balanced')\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "grid.fit(X_train_pca, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con este modelo donde usamos el grid search y cross validation, podemos predecir las clases para los datos de prueba, datos que el modelo no ha visto antes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_ #tome el modelo con los hiperparametros optimos\n",
    "y_pred = model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicciones\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos reales\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos algunas de las imágenes de prueba con sus valores predecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(X_test[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[y_pred[i]].split()[-1],\n",
    "                   color='black' if y_pred[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Nombres predecidos; Nombres predichos mal en rojo', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos datos, nuestro modelo clasificó erróneamente algunas caras (la de Bush la clasificó como Blair). \n",
    "\n",
    "Usamos un reporte de clasificación: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a observar qué es lo que hace el PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para graficar las imagenes\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas de la máquina de vector soporte**\n",
    "\n",
    "- Son modelos muy compactos, usan muy poca memoria\n",
    "- Una vez que el modelo ya está entrenado, la fase de predicción es muy rápida\n",
    "- Como son afectados sólo por los puntos cerca del margen, funcionan muy bien con datos que tienen alta dimensionalidad, aún cuando hay más dimensiones que observaciones. \n",
    "- Su integración con el truco del kernel los hacen métodos muy versátiles, lo cual hace que se adapten a muchos tipos de datos. \n",
    "\n",
    "**Desventajas**\n",
    "- El tiempo de entrenamiento de los datos es alto, por lo que si se tiene una gran cantidad de observaciones, el modelo es muy tardado. \n",
    "- Los resultados son muy dependientes de la variable \"C\", la cual tiene que ser elegida cuidadosamente via cross valitadion, lo cual puede ser costoso computacionalmente si se tienen muchos datos. \n",
    "- Los resultados no tienen una interpretación probabilística, sólo de clasificación. Se puede estimar internamente via cross validation (ver el parámetro de \"probability\" del SVC), pero esta estimación extra es costosa. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
