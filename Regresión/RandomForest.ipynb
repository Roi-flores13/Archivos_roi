{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/1400/1*ZFuMI_HrI3jt2Wlay73IUQ.png\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://miro.medium.com/max/1400/1*ZFuMI_HrI3jt2Wlay73IUQ.png</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Modelos basados en Árboles Parte II</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bosques Aleatorios\n",
    "\n",
    "\"Los árboles tienen un sólo aspecto que previene que sean la herramienta ideal para el aprendizaje predictivo, que es la **inexactitud**\" \n",
    "\n",
    "**Pasos para crear un bosque aleatorio:**\n",
    "- Crear un dataset \"bootstrapped\"\n",
    "- Crear un árbol de decisión usando el dataset \"bootstrapped\", pero sólo usar un subconjunto aleatorio de variables (o columnas) en cada paso. \n",
    "- Regresar al paso 1. y repetir \n",
    "\n",
    "Gracias al proceso de bootstrapping, el requerimento de dividir los datos en prueba y entrenamiento no es tan estricto. Se recomienda dividir los datos en prueba y entrenamiento cuando se quiere comparar su desempeño contra otros modelos. \n",
    "\n",
    "**Hiperparámetros:**\n",
    "- max_depth: Puedo limitar hasta qué profundidad quiero que crezca cada árbol en mi bosque aleatorio.\n",
    "- min_sample_split: le indica al árbol el número mínimo de observaciones requeridas en cualquier nodo dado para dividirlo.\n",
    "- min_samples_leaf: le indica al árbol el número mínimo de observaciones requeridas en la hoja final.\n",
    "- n_estimators: numero de árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos -->\n",
    "# es un ejemplo donde vamos a observar información sobre casas de una inmobiliaria para predecir el precio de renta\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "square_footage = np.random.uniform(800, 4000, n_samples) \n",
    "num_rooms = np.random.randint(2, 7, n_samples)\n",
    "distance_to_city = np.random.uniform(0.5, 30, n_samples)\n",
    "\n",
    "# Variable del precio (no lineal)  y tiene ruido\n",
    "price = (square_footage * 200) + (num_rooms * 5000) - (distance_to_city * 1500)\n",
    "price += np.random.normal(0, 20000, n_samples)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Square Footage': square_footage,\n",
    "    'Number of Rooms': num_rooms,\n",
    "    'Distance to City (Miles)': distance_to_city,\n",
    "    'Price': price\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observar datos\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Tamaño vs precio\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(data['Square Footage'], data['Price'], color='blue')\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Square Footage vs Price')\n",
    "\n",
    "# Numero de cuartos vs precio\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(data['Number of Rooms'], data['Price'], color='green')\n",
    "plt.xlabel('Number of Rooms')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Number of Rooms vs Price')\n",
    "\n",
    "# Distancia ciudad vs precio\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(data['Distance to City (Miles)'], data['Price'], color='red')\n",
    "plt.xlabel('Distance to City (Miles)')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Distance to City vs Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar en train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data[['Price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construir y entrenar el árbol\n",
    "\n",
    "#construir el modelo\n",
    "model = RandomForestRegressor(n_estimators=100,#cuántos árboles queremos\n",
    "                               criterion='squared_error', #error cuadrático\n",
    "                               max_depth=None, #si dejo None puede el árbol ir a su máxima profundidad -> overfitting\n",
    "                               min_samples_split=2, #cuántos datos se necesitan para que haga un split\n",
    "                               min_samples_leaf=2, #cuántos datos dejamos en la última hoja\n",
    "                               bootstrap=True, #¿que haga bootstrapping?\n",
    "                               oob_score=False, \n",
    "                               random_state=0, #semilla\n",
    "                               verbose=0)\n",
    "#calcular cuánto tarda\n",
    "start_time = time.time()\n",
    "\n",
    "#Entrenar el modelo\n",
    "model = model.fit(X_train,y_train.values.ravel()) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Prediciendo en los datos de entrenamiento y prueba \n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print('R2 = %0.4f'%model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(y_test,y_test_pred,c='r',label='data')\n",
    "plt.xlabel('y_real')\n",
    "plt.ylabel('Y_estimated')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si yo quisiera ver sólo un árbol\n",
    "#NO se puede imprimir todo el bosque aleatorio, sólo se puede imprimir árbol por árbol\n",
    "from sklearn import tree\n",
    "tree.plot_tree(model.estimators_[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que está sobreajustando..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridsearchCV para tunear hiperarámetros\n",
    "\n",
    "Utilicemos cross validation para optimizar hiperparámetros. \n",
    "\n",
    "Antes de hacer el GridsearchCV vamos viendo los hiperparámetros cómo se desarrollan contra las métricas de performance para elegir la mejor malla de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos la profundidad vs la R2\n",
    "max_depths = range(1, 10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    tree_reg = RandomForestRegressor(max_depth=max_depth, random_state=42)\n",
    "    tree_reg.fit(X_train, y_train.values.ravel())\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(max_depths, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(max_depths, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('Tree Max Depth')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto de la profundidad del bosque en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos probar la malla entre 1 y 3 profundidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos el min_sample_split vs la R2\n",
    "min_samples_splits = range(1, 30)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    tree_reg = RandomForestRegressor(min_samples_split=min_samples_split, random_state=42)\n",
    "    tree_reg.fit(X_train, y_train.values.ravel())\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(min_samples_splits, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(min_samples_splits, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('Min Sample Split')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto del min. numero de observaciones por split en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos probar la malla entre 15 y 30 min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos el n_estimators vs la R2\n",
    "n_estimators = range(1, 10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    tree_reg = RandomForestRegressor(n_estimators=n_estimator, random_state=42)\n",
    "    tree_reg.fit(X_train, y_train.values.ravel())\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_estimators, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(n_estimators, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('Num Estimators')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto del min. numero del numero de árboles en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar entre 1 y 3 árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando cross validation y grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestRegressor(criterion='squared_error',\n",
    "                               min_samples_leaf=2,\n",
    "                               bootstrap=True,\n",
    "                               oob_score=False,\n",
    "                               random_state=0,\n",
    "                               verbose=0)\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth': range(1, 3), #profundidad máxima\n",
    "                                'min_samples_split': range(15, 30, 1), #minimo numero de observaciones por split\n",
    "                                'n_estimators': range(1,3,1)}, # número de árboles en el bosque\n",
    "                  cv=2,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "#Entrenamiento\n",
    "gs.fit(X_train, y_train.values.ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimir parámetros óptimos\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear modelo usando parámetros óptimos\n",
    "new_model = RandomForestRegressor(n_estimators=2,\n",
    "                               criterion='squared_error',\n",
    "                               max_depth=5,\n",
    "                               min_samples_split=15,\n",
    "                               min_samples_leaf=2,\n",
    "                               bootstrap=True,\n",
    "                               oob_score=False,\n",
    "                               random_state=0,\n",
    "                               verbose=0)\n",
    "#Entrenamiento\n",
    "new_model.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "yhat = new_model.predict(X_test)\n",
    "R2_score = r2_score(y_test,yhat)\n",
    "print('R2:', R2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_score = mean_squared_error(y_test,yhat)\n",
    "print('MSE:', MSE_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver cuáles son las variables más importantes para el modelo\n",
    "new_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols=['var1','var2','var3']\n",
    "(pd.Series(new_model.feature_importances_, index=cols)\n",
    "   .nlargest(3)\n",
    "   .plot(kind='barh')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas**\n",
    "\n",
    "- Son muy buenos generalizando\n",
    "- Protejen en contra del sobreajuste (overfitting) gracias a la construcción del bootstrapping \n",
    "- También reducen la varianza y por lo tanto mejoran la precisión del modelo\n",
    "- Funcionan muy bien con variables categóricas y variables continuas\n",
    "- No se requiere escalamiento previo de variables \n",
    "- Manejan muy bien el hecho de que haya datos nulos\n",
    "- Son modelos robustos ante valores atípicos (outliers)\n",
    "- Son algoritmos muy estables, cuando hay datos nuevos, el algoritmo no se ve muy afectado. Ya que este nuevo dato puede afectar a un árbol individual, pero es difícil que impacte a todos los árboles. \n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "- Complejidad. Los bosques aleatorios crean muchos árboles y combina sus resultados. Requiere mucho poder computacional y recursos \n",
    "- Periodos de entrenamiento largos. Requieren más tiempo de entrenamiento. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
