{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://bernardmarr.com/img/What%20is%20an%20Artificial%20Neural%20Networks.jpg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://bernardmarr.com/img/What%20is%20an%20Artificial%20Neural%20Networks.jpg</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Redes Neuronales</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales profundas (deep learning) han ganado mucha popularidad en los últimos años, especialmente para problemas complejos, como el reconocimiento de imágenes, el procesamiento del lenguaje natural y otras aplicaciones que requieren modelos potentes. \n",
    "\n",
    "Sin embargo, las redes neuronales básicas también pueden usarse en problemas de aprendizaje supervisado más simples, como regresión o clasificación, AUNQUE hay varias razones por las cuales otros algoritmos son más comunes en estos casos...\n",
    "\n",
    "1. Complejidad computacional\n",
    "2. Sobreajuste (Overfitting)\n",
    "3. Necesidad de más datos\n",
    "4. Tuning de Hiperparámetros (las redes neuronales tienen muchos hiperparámetros)\n",
    "5. Interpretabilidad, son modelos de caja negra\n",
    "\n",
    "**¿Cuándo se utilizan redes neuronales en aprendizaje supervisado?**\n",
    "\n",
    "Aunque no siempre son la primera opción para problemas simples, las redes neuronales pueden ser muy útiles en problemas supervisados más complejos donde:\n",
    "\n",
    "- Hay gran cantidad de datos disponibles\n",
    "- Los problemas son no lineales\n",
    "- Hay tareas de percepción involucradas (como visión por computadora, reconocimiento de voz o procesamiento de lenguaje natural), donde las redes neuronales profundas sobresalen.\n",
    "\n",
    "**Continuando...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='compare_NN.JPG', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la caja negra...\n",
    "\n",
    "**¿Qué es una neurona?**\n",
    "\n",
    "Es un nombre elegante que le pusieron para referirse a una función en los años de 1940-1950. Se creeía que los nodos eran como neuronas del cerebro.  \n",
    "\n",
    "**¿Qué es una función?**\n",
    "\n",
    "Es un nombre elegante que pusieron para referirse a algo que toma cosas de entrada, aplica alguna lógica y saca un resultado. \n",
    "\n",
    "**¿Qué es una red neuronal?**\n",
    "\n",
    "Es la interconexión de múltiples neuronas (funciones) para resolver un problema complejo que una sola neurona no podría. Dentro de los componentes existen *nodos* y *conexiones*\n",
    "\n",
    "**Pasos que hace la red neuronal:**\n",
    "\n",
    "1. Combinación de las variables de entrada (X). Cada señal recibida tiene una constante w (peso/weight) que permite manipular la importancia de cada variable de entrada. \n",
    "2. Neurona de respuesta. Considerando todas las variables de entrada, la neurona genera una señal de respuesta a través de una función. \n",
    "\n",
    "**Tipos de capas en la red neuronal**\n",
    "\n",
    "- Capa de entrada: capa que toma los datos de entrada\n",
    "- Capas ocultas: toman las entradas de otra capa y pasan a salida hacia otra capa (son intermediarias entre la entrada y la salida)\n",
    "- Capas de salida: capa que hace la predicción\n",
    "\n",
    "<img style=\"float: center; margin: 0px 0px 0px 0px;\" src=\"https://www.smartsheet.com/sites/default/files/IC-simplified-artificial-neural-networks-corrected.svg\" width=\"350px\" height=\"180px\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='neuron_weights.JPG', width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo matemático de la neurona**\n",
    "\n",
    "$$\\nu = w_{0}+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}+w_{4}x_{4}+w_{5}x_{5}$$\n",
    "$$y = \\varphi(\\nu)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='capa_oculta.JPG', width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo matemático red neuronal**\n",
    "\n",
    "$$\\nu^{1} = w_{0}^{1}+w^{1}$$\n",
    "$$y^{1} = \\varphi(\\nu^{1})$$\n",
    "$$\\nu^{2} = w_{0}^{2}+w^{2}y_{1}$$\n",
    "$$y^{2} = \\varphi(\\nu^{2})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='capa_oculta2.JPG', width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo matemático usando dos capas oculta**\n",
    "\n",
    "<font color= #66CC00>$$\\nu^{1} = w_{0}^{1}+w^{1}$$</font>\n",
    "<font color= #66CC00>$$y^{1} = \\varphi(\\nu^{1})$$</font>\n",
    "<font color= #009900>$$\\nu^{2} = w_{0}^{2}+w^{2}y_{1}$$</font>\n",
    "<font color= #009900>$$y^{2} = \\varphi(\\nu^{2})$$</font>\n",
    "<font color= #3399FF>$$\\nu^{3} = w_{0}^{3}+w^{3}y_{2}$$</font>\n",
    "<font color= #3399FF>$$y^{3} = \\varphi(\\nu^{3})$$</font>\n",
    "\n",
    "\n",
    "**Funciones de activación**\n",
    "\n",
    "El comportamiento de las neuronas de las redes neuronales es basado en la función en la que la neurona se especializó. Una neurona puede cambiar su respuesta dependiendo de la función de activación $\\varphi(*)$ que sea seleccionada\n",
    "\n",
    "Funciones comunes:\n",
    "1. Lineal: $\\varphi(\\nu)=\\nu$\n",
    "2. Sigmoidal: $\\varphi(\\nu)=\\frac{e^{\\nu}}{1+e^{\\nu}}$ \n",
    "3. Tanh: $\\varphi(\\nu)=\\frac{e^{\\nu}-e^{-\\nu}}{e^{\\nu}+e^{-\\nu}}=tanh(\\nu)$\n",
    "4. Softplus: $\\varphi(\\nu)=log(1+e^{\\nu})$\n",
    "5. RELU: $\\varphi(\\nu)=max(0,\\nu)$\n",
    "\n",
    "Las funciones de activación suelen ser diferenciables, lo que significa que la derivada de primer orden se puede calcular para un valor de entrada dado. Esto es necesario dado que las redes neuronales generalmente se entrenan utilizando el algoritmo de backpropagation que requiere la derivada del error de predicción para actualizar los pesos del modelo.\n",
    "\n",
    "**¿Cómo elegir la función de activación para las capas ocultas?**\n",
    " \n",
    "- RELU: es la más utilizada para capas ocultas. Es fácil de calcular, lo que la hace más rápida para entrenar. Es simple de implementar y no es tan susceptible a gradientes que se desvanecen\n",
    "- Leaky RELU: Similar a ReLU, pero permite valores negativos pequeños cuando x≤0, lo que evita que las neuronas queden completamente inactivas.\n",
    "- Evita Sigmoid y Tanh en las capas ocultas\n",
    "\n",
    "**¿Cómo elegir la función de activación para las capas de salida?**\n",
    "\n",
    "- Lineal: Para problemas de regresión donde la salida es un valor continuo.\n",
    "- Sigmoidal: típicamente para problemas de clasificación binaria.\n",
    "- Softmax: Para problemas de clasificación multiclase\n",
    "\n",
    "\n",
    "**¿Cómo son las redes neuronales para regresión?**\n",
    "\n",
    "$$\\nu^{1} = w_{0}^{1}+w^{1}$$\n",
    "$$y^{1} = \\varphi(\\nu^{1})$$\n",
    "$$\\nu^{2} = w_{0}^{2}+w^{2}y_{1}$$\n",
    "<font color= #2E9AFE>$$y^{2} = \\varphi(\\nu^{2})$$</font>\n",
    "\n",
    "La única diferencia es la última salida. La función de la salida tiene que ser **lineal**\n",
    "\n",
    "\n",
    "**¿Cómo elegir cuántas capas ocultas?**\n",
    "\n",
    "- Problemas sencillos: Una sola capa oculta suele ser suficiente para capturar patrones simples.\n",
    "- Problemas más complejos (con relaciones no lineales más complicadas): Dos capas ocultas pueden ser útiles \n",
    "\n",
    "En la práctica, la mayoría de los problemas sencillos no necesitan más de 1 o 2 capas ocultas.\n",
    "\n",
    "**¿Cómo elegir cuántas neuronas en las capas ocultas?**\n",
    "\n",
    "- Un buen punto de partida es comenzar con un número de neuronas que sea similar o cercano al número de características de entrada.\n",
    "- Para problemas simples, se puedes comenzar con entre 4 y 16 neuronas por capa.\n",
    "- Si los datos tienen una cantidad moderada de complejidad o son no lineales, podrías aumentar a 32 o 64 neuronas en la capa oculta.\n",
    "- Evita tener muchas más neuronas que características de entrada, ya que eso puede llevar a un sobreajuste.\n",
    "- En redes con múltiples capas ocultas, una estrategia común es que las capas posteriores tengan menos neuronas que las capas iniciales. Esto es porque las primeras capas extraen más características, mientras que las capas más profundas se enfocan en combinarlas.\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Es el método que usa para ajustar el modelo. Minimiza el error medio cuadrático (MSE), utilizando el gradiente descendente para calcular los parámetros con el orden del último hacia el primero.\n",
    "\n",
    "**Recomendaciones**\n",
    "\n",
    "- Se recomienda hacer un pre-procesamiento de los datos - Escalamiento\n",
    "- Comúnmente todas las capas ocultas usan la misma función de activación\n",
    "- La función de salida generalmente usa diferente función de activación que el de las capas ocultas. \n",
    "- En la experiencia, conviene más aumentar el número de neuronas ocultas en lugar de aumentar el número de capas ocultas.\n",
    "- Se recomienda no usar muchas capas ocultas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sintéticos (relación no lineal)\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "#graficando los datos\n",
    "fig = plt.figure()\n",
    "plt.plot(X,y, linewidth=3)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0,test_size=0.25)\n",
    "\n",
    "# Normalizar los datos (muy recomendable para las redes neuronales)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuántos datos tenemos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las recomendaciones es *Si los datos tienen una cantidad moderada de complejidad o son no lineales, podrías aumentar a 32 o 64 neuronas en la capa oculta.*\n",
    "\n",
    "Otra recomendación es *Un buen punto de partida es comenzar con un número de neuronas que sea similar o cercano al número de características de entrada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de red neuronal\n",
    "model = Sequential() #inicializamos el modelo\n",
    "model.add(Dense(, input_dim=1, activation=''))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(, activation='relu'))  # Segunda capa oculta con 64 neuronas y ReLU\n",
    "model.add(Dense(1, activation='linear'))  # Capa de salida con una sola neurona (para regresión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Visualización de los resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gráfico de datos de entrenamiento\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='blue', label='Datos de entrenamiento')\n",
    "plt.scatter(X_train, y_pred_train, color='red', label='Predicción de la red')\n",
    "plt.title('Entrenamiento')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de datos de prueba\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='green', label='Datos de prueba')\n",
    "plt.scatter(X_test, y_pred_test, color='red', label='Predicción de la red')\n",
    "plt.title('Prueba')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de la pérdida durante el entrenamiento\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Pérdida en entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida en prueba')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2\n",
    "r2_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross Validation\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos del conjunto de diabetes\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función que cree la arquitectura de la red neuronal\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Envolver el modelo con KerasRegressor\n",
    "model = KerasRegressor(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Crear un pipeline que primero escala los datos y luego entrena el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalar los datos\n",
    "    ('model', model)               # Entrenar el modelo\n",
    "])\n",
    "\n",
    "# Configurar validación cruzada con 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluar el modelo para MSE\n",
    "results_mse = -cross_val_score(pipeline, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Evaluar el modelo para R2\n",
    "results_r2 = cross_val_score(pipeline, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "# Convertimos a valores positivos para interpretación del error\n",
    "print(f\"Promedio de MSE positivo: {np.mean(results_mse)}\")\n",
    "\n",
    "# Mostrar los resultados de la validación cruzada para R2\n",
    "print(f\"Promedio de R2: {np.mean(results_r2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas y desventajas de las redes neuronales\n",
    "\n",
    "#### Ventajas\n",
    "\n",
    "- Capacidad de trabajar con relaciones no lineales\n",
    "- Pueden procesar datos no estructurados\n",
    "- Capacidad de procesamiento en paralelo\n",
    "- Autoorganización: la capacidad de agrupar y clasificar grandes cantidades de datos hace que las redes neuronales sean especialmente adecuadas para organizar los complicados problemas visuales que plantea el análisis de imágenes\n",
    "\n",
    "#### Desventajas\n",
    "\n",
    "- Debido a su naturaleza de \"caja negra\" se pierde interpretación del modelo\n",
    "- Usualmente requieren muchos más datos que los modelos tradicionales\n",
    "- Si hay demasiados datos, la red neuronal requiere procesadores con gran poder de procesamiento\n",
    "- No hay regla específica para determinar la estructura de la red - Prueba y error\n",
    "- Pueden llegar a convertirse muy complejas\n",
    "- Tienden a sobreajustar (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algunas referencias\n",
    "\n",
    "- https://keras.io/guides/sequential_model/\n",
    "- https://keras.io/api/layers/activations/\n",
    "- https://keras.io/optimizers/\n",
    "- https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0.003&noise=20&networkShape=4&seed=0.82538&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
